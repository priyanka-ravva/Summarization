{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method4 [ https://pypi.org/project/rouge-metric/ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================ Loading json files ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: Samples count = 26917 \n"
     ]
    }
   ],
   "source": [
    "#### Loading json files:\n",
    "\n",
    "import json\n",
    "\n",
    "f = open('rouge_scores.json',) \n",
    "samples_ngrams_scores = json.load(f) \n",
    "\n",
    "\n",
    "f2 = open(\"dataset.json\",) \n",
    "dataset = json.load(f2) \n",
    "\n",
    "\n",
    "print(\"Corpus: Samples count = %d \"%len(dataset.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: Samples count = 26917 \n",
      "-------------------------------------------\n",
      "\n",
      "articles sentences>=3, samples counts  = 26917 \n",
      "summaries sentences>=3, samples counts  = 15089 \n",
      "-------------------------------------------\n",
      "\n",
      "Sentence_CR <=50, samples counts  = 21474 \n",
      "==============================================\n",
      "\n",
      "\n",
      "Tokens_CR <=50, samples counts  = 9772 \n",
      "-------------------------------------------\n",
      "\n",
      "Tokens_CR <=60, samples counts  = 15871 \n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Corpus: Samples count = %d \"%len(dataset.keys()))\n",
    "print(\"-------------------------------------------\\n\")\n",
    "filter_samples = [sample for sample in dataset.keys() if( len(dataset[sample]['article']['sentences_wise'])>=3   )]\n",
    "print(\"articles sentences>=3, samples counts  = %d \"%len(filter_samples))\n",
    "\n",
    "filter_samples = [sample for sample in dataset.keys() if( len(dataset[sample]['summary']['sentences_wise'])>=3   )]\n",
    "print(\"summaries sentences>=3, samples counts  = %d \"%len(filter_samples))\n",
    "print(\"-------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "filter_samples = [sample for sample in dataset.keys() if(((len(dataset[sample]['summary']['sentences_wise']))/float(len(dataset[sample]['article']['sentences_wise'])) )*100)<=50]\n",
    "print(\"Sentence_CR <=50, samples counts  = %d \"%len(filter_samples))\n",
    "print(\"==============================================\\n\\n\")\n",
    "\n",
    "filter_samples = [sample for sample in dataset.keys() if(((len(dataset[sample]['summary']['tokens_wise']))/float(len(dataset[sample]['article']['tokens_wise'])) )*100)<=50]\n",
    "print(\"Tokens_CR <=50, samples counts  = %d \"%len(filter_samples))\n",
    "print(\"-------------------------------------------\\n\")\n",
    "\n",
    "filter_samples = [sample for sample in dataset.keys() if(((len(dataset[sample]['summary']['tokens_wise']))/float(len(dataset[sample]['article']['tokens_wise'])) )*100)<=60]\n",
    "print(\"Tokens_CR <=60, samples counts  = %d \"%len(filter_samples))\n",
    "print(\"-------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples count = 26917 \n",
      "\n",
      "Summaries Length in terms of sentences: \n",
      "trigrams novelty greater than 25 percentages samples count = 22301\n",
      "summary sentences len >=3 and trigrams novelty >=25 percentages samples count = 12165\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "\n",
      "Compression Ratio(CR) in terms of Sentences:\n",
      "\n",
      "trigrams novelty greater than 25 percentages samples count = 22301\n",
      "sentences_CR <=50 and trigrams novelty >=25 percentages samples count = 18202\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "Compression Ratio(CR) in terms of Tokens:\n",
      "\n",
      "trigrams novelty greater than 25 percentages samples count = 22301\n",
      "tokens_CR <=50 and trigrams novelty >=25 percentages samples count = 8668\n",
      "tokens_CR <=60 and trigrams novelty >=25 percentages samples count = 13865\n",
      "tokens_CR <=70 and trigrams novelty >=25 percentages samples count = 18166\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "Summaries Length in terms of sentences: \n",
      "bigrams novelty greater than 25 percentages samples count = 17724\n",
      "summary sentences len >=3 and bigrams novelty >=25 percentages samples count = 9715\n",
      "\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "Compression Ration(CR) in terms of Sentence:\n",
      "\n",
      "bigrams novelty greater than 25 percentages samples count = 17724\n",
      "sentences_CR <=50 and bigrams novelty >=25 percentages samples count = 14483\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "Compression Ration(CR) in terms of Tokens:\n",
      "\n",
      "bigrams novelty greater than 25 percentages samples count = 17724\n",
      "tokens_CR <=50 and bigrams novelty >=25 percentages samples count = 7333\n",
      "tokens_CR <=60 and bigrams novelty >=25 percentages samples count = 11473\n",
      "tokens_CR <=70 and bigrams novelty >=25 percentages samples count = 14776\n",
      "\n",
      "---------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"samples count = %d \"%len(dataset.keys()))\n",
    "list_samples = list(samples_ngrams_scores.keys())\n",
    "\n",
    "#scores: samples_ngrams_scores\n",
    "#dataset: dataset\n",
    "\n",
    "\n",
    "print(\"\\nSummaries Length in terms of sentences: \")\n",
    "trigrams = [ sample for sample in list_samples if(samples_ngrams_scores[sample]['novelty']['rouge-3']>=25) ] \n",
    "print(\"trigrams novelty greater than 25 percentages samples count = %d\"%len(trigrams))\n",
    "\n",
    "trigrams = [ sample for sample in list_samples if( (samples_ngrams_scores[sample]['novelty']['rouge-3']>=25) and(len(dataset[sample]['summary']['sentences_wise'])>=3) )] \n",
    "print(\"summary sentences len >=3 and trigrams novelty >=25 percentages samples count = %d\"%len(trigrams))\n",
    "print(\"\\n---------------------------------------------------------\\n\")    \n",
    "\n",
    "\n",
    "print(\"\\nCompression Ratio(CR) in terms of Sentences:\\n\")\n",
    "trigrams = [ sample for sample in list_samples if(samples_ngrams_scores[sample]['novelty']['rouge-3']>=25) ] \n",
    "print(\"trigrams novelty greater than 25 percentages samples count = %d\"%len(trigrams))\n",
    "\n",
    "trigrams = [ sample for sample in list_samples if( (samples_ngrams_scores[sample]['novelty']['rouge-3']>=25) and( (len(dataset[sample]['summary']['sentences_wise'])/float(len(dataset[sample]['article']['sentences_wise'])) )*100  )<=50 )] \n",
    "print(\"sentences_CR <=50 and trigrams novelty >=25 percentages samples count = %d\"%len(trigrams))\n",
    "print(\"\\n---------------------------------------------------------\\n\")    \n",
    "\n",
    "\n",
    "print(\"Compression Ratio(CR) in terms of Tokens:\\n\")\n",
    "trigrams = [ sample for sample in list_samples if(samples_ngrams_scores[sample]['novelty']['rouge-3']>=25) ] \n",
    "print(\"trigrams novelty greater than 25 percentages samples count = %d\"%len(trigrams))\n",
    "\n",
    "trigrams = [ sample for sample in list_samples if( (samples_ngrams_scores[sample]['novelty']['rouge-3']>=25) and( (len(dataset[sample]['summary']['tokens_wise'])/float(len(dataset[sample]['article']['tokens_wise'])) )*100  )<=50 )] \n",
    "print(\"tokens_CR <=50 and trigrams novelty >=25 percentages samples count = %d\"%len(trigrams))\n",
    "#print(\"\\n---------------------------------------\\n\")    \n",
    "\n",
    "trigrams = [ sample for sample in list_samples if(samples_ngrams_scores[sample]['novelty']['rouge-3']>=25) ] \n",
    "#print(\"trigrams novelty greater than 25 percentages samples count = %d\"%len(trigrams))\n",
    "\n",
    "trigrams = [ sample for sample in list_samples if( (samples_ngrams_scores[sample]['novelty']['rouge-3']>=25) and( (len(dataset[sample]['summary']['tokens_wise'])/float(len(dataset[sample]['article']['tokens_wise'])) )*100  )<=60 )] \n",
    "print(\"tokens_CR <=60 and trigrams novelty >=25 percentages samples count = %d\"%len(trigrams))\n",
    "#print(\"\\n---------------------------------------\\n\")    \n",
    "\n",
    "\n",
    "trigrams = [ sample for sample in list_samples if(samples_ngrams_scores[sample]['novelty']['rouge-3']>=25) ] \n",
    "#print(\"trigrams novelty greater than 25 percentages samples count = %d\"%len(trigrams))\n",
    "\n",
    "trigrams = [ sample for sample in list_samples if( (samples_ngrams_scores[sample]['novelty']['rouge-3']>=25) and( (len(dataset[sample]['summary']['tokens_wise'])/float(len(dataset[sample]['article']['tokens_wise'])) )*100  )<=70 )] \n",
    "print(\"tokens_CR <=70 and trigrams novelty >=25 percentages samples count = %d\"%len(trigrams))\n",
    "print(\"\\n==========================================================================================\\n\")    \n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nSummaries Length in terms of sentences: \")\n",
    "bigrams = [ sample for sample in list_samples if(samples_ngrams_scores[sample]['novelty']['rouge-2']>=25) ] \n",
    "print(\"bigrams novelty greater than 25 percentages samples count = %d\"%len(bigrams))\n",
    "bigrams = [ sample for sample in list_samples if( (samples_ngrams_scores[sample]['novelty']['rouge-2']>=25) and(len(dataset[sample]['summary']['sentences_wise'])>=3) )] \n",
    "print(\"summary sentences len >=3 and bigrams novelty >=25 percentages samples count = %d\"%len(bigrams))\n",
    "print(\"\\n--------------------------------------------------------\\n\")    \n",
    "\n",
    "\n",
    "print(\"\\nCompression Ration(CR) in terms of Sentence:\\n\")\n",
    "bigrams = [ sample for sample in list_samples if(samples_ngrams_scores[sample]['novelty']['rouge-2']>=25) ] \n",
    "print(\"bigrams novelty greater than 25 percentages samples count = %d\"%len(bigrams))\n",
    "\n",
    "bigrams =  [ sample for sample in list_samples if( (samples_ngrams_scores[sample]['novelty']['rouge-2']>=25) and( (len(dataset[sample]['summary']['sentences_wise'])/float(len(dataset[sample]['article']['sentences_wise'])) )*100  )<=50 )] \n",
    "print(\"sentences_CR <=50 and bigrams novelty >=25 percentages samples count = %d\"%len(bigrams))\n",
    "print(\"\\n-------------------------------------------------------\\n\")\n",
    "\n",
    "print(\"Compression Ration(CR) in terms of Tokens:\\n\")\n",
    "bigrams = [ sample for sample in list_samples if(samples_ngrams_scores[sample]['novelty']['rouge-2']>=25) ] \n",
    "print(\"bigrams novelty greater than 25 percentages samples count = %d\"%len(bigrams))\n",
    "\n",
    "bigrams =  [ sample for sample in list_samples if( (samples_ngrams_scores[sample]['novelty']['rouge-2']>=25) and( (len(dataset[sample]['summary']['tokens_wise'])/float(len(dataset[sample]['article']['tokens_wise'])) )*100  )<=50 )] \n",
    "print(\"tokens_CR <=50 and bigrams novelty >=25 percentages samples count = %d\"%len(bigrams))\n",
    "#print(\"\\n---------------------------------------\\n\")    \n",
    "bigrams = [ sample for sample in list_samples if(samples_ngrams_scores[sample]['novelty']['rouge-2']>=25) ] \n",
    "#print(\"bigrams novelty greater than 25 percentages samples count = %d\"%len(bigrams))\n",
    "\n",
    "bigrams =  [ sample for sample in list_samples if( (samples_ngrams_scores[sample]['novelty']['rouge-2']>=25) and( (len(dataset[sample]['summary']['tokens_wise'])/float(len(dataset[sample]['article']['tokens_wise'])) )*100  )<=60 )] \n",
    "print(\"tokens_CR <=60 and bigrams novelty >=25 percentages samples count = %d\"%len(bigrams))\n",
    "#print(\"\\n---------------------------------------\\n\")    \n",
    "\n",
    "bigrams = [ sample for sample in list_samples if(samples_ngrams_scores[sample]['novelty']['rouge-2']>=25) ] \n",
    "#print(\"bigrams novelty greater than 25 percentages samples count = %d\"%len(bigrams))\n",
    "\n",
    "bigrams =  [ sample for sample in list_samples if( (samples_ngrams_scores[sample]['novelty']['rouge-2']>=25) and( (len(dataset[sample]['summary']['tokens_wise'])/float(len(dataset[sample]['article']['tokens_wise'])) )*100  )<=70 )] \n",
    "print(\"tokens_CR <=70 and bigrams novelty >=25 percentages samples count = %d\"%len(bigrams))\n",
    "print(\"\\n---------------------------------------\\n\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bucketing: Data Distribution \n",
    "\n",
    "### Threshold : ( [25<= Novelty <=95] & [ token_CR <=60 ] ) ====> Samples_count \n",
    "\n",
    "####  tokens_CR bins = [10, 20, 30, 40, 50 , 60 ]; Novelty_bins = [ 35, 45, 55, 65, 75, 85, 95 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intially tokens_CR_and_novelty_bins :  {10: {35: [], 45: [], 55: [], 65: [], 75: [], 85: [], 95: []}, 20: {35: [], 45: [], 55: [], 65: [], 75: [], 85: [], 95: []}, 30: {35: [], 45: [], 55: [], 65: [], 75: [], 85: [], 95: []}, 40: {35: [], 45: [], 55: [], 65: [], 75: [], 85: [], 95: []}, 50: {35: [], 45: [], 55: [], 65: [], 75: [], 85: [], 95: []}, 60: {35: [], 45: [], 55: [], 65: [], 75: [], 85: [], 95: []}}\n",
      "\n",
      "\n",
      "\n",
      "cr_bin_key = 10 , novelty_bin_key = 35 , bin_samples_count = 0 , total_samples_counts = 0\n",
      "cr_bin_key = 10 , novelty_bin_key = 45 , bin_samples_count = 1 , total_samples_counts = 1\n",
      "cr_bin_key = 10 , novelty_bin_key = 55 , bin_samples_count = 0 , total_samples_counts = 1\n",
      "cr_bin_key = 10 , novelty_bin_key = 65 , bin_samples_count = 1 , total_samples_counts = 2\n",
      "cr_bin_key = 10 , novelty_bin_key = 75 , bin_samples_count = 1 , total_samples_counts = 3\n",
      "cr_bin_key = 10 , novelty_bin_key = 85 , bin_samples_count = 1 , total_samples_counts = 4\n",
      "cr_bin_key = 10 , novelty_bin_key = 95 , bin_samples_count = 1 , total_samples_counts = 5\n",
      "--------------------------------\n",
      "cr_bin_key = 20 , novelty_bin_key = 35 , bin_samples_count = 9 , total_samples_counts = 14\n",
      "cr_bin_key = 20 , novelty_bin_key = 45 , bin_samples_count = 7 , total_samples_counts = 21\n",
      "cr_bin_key = 20 , novelty_bin_key = 55 , bin_samples_count = 11 , total_samples_counts = 32\n",
      "cr_bin_key = 20 , novelty_bin_key = 65 , bin_samples_count = 16 , total_samples_counts = 48\n",
      "cr_bin_key = 20 , novelty_bin_key = 75 , bin_samples_count = 36 , total_samples_counts = 84\n",
      "cr_bin_key = 20 , novelty_bin_key = 85 , bin_samples_count = 46 , total_samples_counts = 130\n",
      "cr_bin_key = 20 , novelty_bin_key = 95 , bin_samples_count = 34 , total_samples_counts = 164\n",
      "--------------------------------\n",
      "cr_bin_key = 30 , novelty_bin_key = 35 , bin_samples_count = 74 , total_samples_counts = 238\n",
      "cr_bin_key = 30 , novelty_bin_key = 45 , bin_samples_count = 83 , total_samples_counts = 321\n",
      "cr_bin_key = 30 , novelty_bin_key = 55 , bin_samples_count = 96 , total_samples_counts = 417\n",
      "cr_bin_key = 30 , novelty_bin_key = 65 , bin_samples_count = 134 , total_samples_counts = 551\n",
      "cr_bin_key = 30 , novelty_bin_key = 75 , bin_samples_count = 131 , total_samples_counts = 682\n",
      "cr_bin_key = 30 , novelty_bin_key = 85 , bin_samples_count = 152 , total_samples_counts = 834\n",
      "cr_bin_key = 30 , novelty_bin_key = 95 , bin_samples_count = 144 , total_samples_counts = 978\n",
      "--------------------------------\n",
      "cr_bin_key = 40 , novelty_bin_key = 35 , bin_samples_count = 322 , total_samples_counts = 1300\n",
      "cr_bin_key = 40 , novelty_bin_key = 45 , bin_samples_count = 349 , total_samples_counts = 1649\n",
      "cr_bin_key = 40 , novelty_bin_key = 55 , bin_samples_count = 415 , total_samples_counts = 2064\n",
      "cr_bin_key = 40 , novelty_bin_key = 65 , bin_samples_count = 437 , total_samples_counts = 2501\n",
      "cr_bin_key = 40 , novelty_bin_key = 75 , bin_samples_count = 459 , total_samples_counts = 2960\n",
      "cr_bin_key = 40 , novelty_bin_key = 85 , bin_samples_count = 388 , total_samples_counts = 3348\n",
      "cr_bin_key = 40 , novelty_bin_key = 95 , bin_samples_count = 320 , total_samples_counts = 3668\n",
      "--------------------------------\n",
      "cr_bin_key = 50 , novelty_bin_key = 35 , bin_samples_count = 634 , total_samples_counts = 4302\n",
      "cr_bin_key = 50 , novelty_bin_key = 45 , bin_samples_count = 810 , total_samples_counts = 5112\n",
      "cr_bin_key = 50 , novelty_bin_key = 55 , bin_samples_count = 777 , total_samples_counts = 5889\n",
      "cr_bin_key = 50 , novelty_bin_key = 65 , bin_samples_count = 809 , total_samples_counts = 6698\n",
      "cr_bin_key = 50 , novelty_bin_key = 75 , bin_samples_count = 699 , total_samples_counts = 7397\n",
      "cr_bin_key = 50 , novelty_bin_key = 85 , bin_samples_count = 544 , total_samples_counts = 7941\n",
      "cr_bin_key = 50 , novelty_bin_key = 95 , bin_samples_count = 390 , total_samples_counts = 8331\n",
      "--------------------------------\n",
      "cr_bin_key = 60 , novelty_bin_key = 35 , bin_samples_count = 840 , total_samples_counts = 9171\n",
      "cr_bin_key = 60 , novelty_bin_key = 45 , bin_samples_count = 947 , total_samples_counts = 10118\n",
      "cr_bin_key = 60 , novelty_bin_key = 55 , bin_samples_count = 979 , total_samples_counts = 11097\n",
      "cr_bin_key = 60 , novelty_bin_key = 65 , bin_samples_count = 860 , total_samples_counts = 11957\n",
      "cr_bin_key = 60 , novelty_bin_key = 75 , bin_samples_count = 710 , total_samples_counts = 12667\n",
      "cr_bin_key = 60 , novelty_bin_key = 85 , bin_samples_count = 492 , total_samples_counts = 13159\n",
      "cr_bin_key = 60 , novelty_bin_key = 95 , bin_samples_count = 295 , total_samples_counts = 13454\n",
      "--------------------------------\n",
      "\n",
      "========================================================\n",
      "\n",
      "Novelty >=25 percentages samples count = 22301\n",
      "tokens_CR <=60 percentages samples count = 15871\n",
      "tokens_CR <=60 and trigrams novelty >=25 percentages samples count = 13865\n",
      "\n",
      "\n",
      "Filitered_samples threshold is (25<= Novelty <=95) and (token_CR <=60) \n",
      "corpus_size = 26917,  Filitered_samples = 13454 \n"
     ]
    }
   ],
   "source": [
    "#scores: samples_ngrams_scores\n",
    "#dataset: dataset\n",
    "\n",
    "list_samples = dataset.keys()\n",
    "\n",
    "Novelty_bins = {i:[] for i in range(35, 106, 10)} #[ 35, 45, 55, 65, 75, 85, 95]\n",
    "tokens_CR_bins = {i:[] for i in range(10, 70, 10)}#{10, 20, 30, 40, 50 , 60 }\n",
    "\n",
    "\n",
    "novelty_max_limit = 100\n",
    "\n",
    "tokens_CR_and_novelty_bins = {i:{j:[] for j in range(35, novelty_max_limit, 10)} for i in range(10, 70, 10)}#{10, 20, 30, 40, 50 , 60 }\n",
    "print(\"intially tokens_CR_and_novelty_bins : \",tokens_CR_and_novelty_bins)\n",
    "\n",
    "#print(tokens_CR_bins)\n",
    "#print(Novelty_bins)\n",
    "#print(tokens_CR_and_novelty_bins)\n",
    "\n",
    "\n",
    "\n",
    "########################### distributing the samples ################\n",
    "for sample in list_samples:\n",
    "    sample_token_cr = (len(dataset[sample]['summary']['tokens_wise'])/float(len(dataset[sample]['article']['tokens_wise'])))*100\n",
    "    sample_trigram_novelty_score = samples_ngrams_scores[sample]['novelty']['rouge-3']\n",
    "    if(sample_token_cr<=60 and sample_trigram_novelty_score>=25 and sample_trigram_novelty_score<=95):\n",
    "\n",
    "        #print(sample_token_cr, sample_trigram_novelty_score)\n",
    "\n",
    "        flag1 = False\n",
    "        flag2 = False\n",
    "        #### checking for CR bin:\n",
    "        for cr_bin in tokens_CR_bins:\n",
    "            if(sample_token_cr<=cr_bin):\n",
    "                tokens_CR_bins[cr_bin].append(sample)\n",
    "                flag1 = True\n",
    "                #### checking for novelty_bin:\n",
    "                for novel_bin in Novelty_bins:\n",
    "                    if(sample_trigram_novelty_score<=novel_bin):\n",
    "                        Novelty_bins[novel_bin].append(sample)\n",
    "                        flag2 = True\n",
    "                        tokens_CR_and_novelty_bins[cr_bin][novel_bin].append(sample)\n",
    "                        break\n",
    "                        \n",
    "                    \n",
    "            if(flag1 and flag2):\n",
    "                break\n",
    "                    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "sc = 0\n",
    "print(\"\\n\\n\")\n",
    "for cr_bin_key in tokens_CR_and_novelty_bins.keys():\n",
    "    for novelty_bin_key in range(35, novelty_max_limit, 10):\n",
    "        temp=len(tokens_CR_and_novelty_bins[cr_bin_key][novelty_bin_key])\n",
    "        sc+=temp\n",
    "        print(\"cr_bin_key = %d , novelty_bin_key = %d , bin_samples_count = %d , total_samples_counts = %d\"%(cr_bin_key,novelty_bin_key, temp, sc ) )\n",
    "        #print(temp)\n",
    "        #t = temp\n",
    "        #print(t/3)\n",
    "    \n",
    "    \n",
    "    print(\"--------------------------------\")\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "print(\"\\n========================================================\\n\")\n",
    "trigrams = [ sample for sample in list_samples if(samples_ngrams_scores[sample]['novelty']['rouge-3']>=25)] \n",
    "print(\"Novelty >=25 percentages samples count = %d\"%len(trigrams))\n",
    "\n",
    "\n",
    "\n",
    "trigrams = [ sample for sample in list_samples if( ((len(dataset[sample]['summary']['tokens_wise'])/float(len(dataset[sample]['article']['tokens_wise'])) )*100)<=60 )] \n",
    "print(\"tokens_CR <=60 percentages samples count = %d\"%len(trigrams))\n",
    "\n",
    "\n",
    "trigrams = [ sample for sample in list_samples if( (samples_ngrams_scores[sample]['novelty']['rouge-3']>=25) and( (len(dataset[sample]['summary']['tokens_wise'])/float(len(dataset[sample]['article']['tokens_wise'])) )*100  )<=60 )] \n",
    "print(\"tokens_CR <=60 and trigrams novelty >=25 percentages samples count = %d\"%len(trigrams))\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Filitered_samples threshold is (25<= Novelty <=95) and (token_CR <=60) \")\n",
    "print(\"corpus_size = %d,  Filitered_samples = %d \"%(len(list_samples), sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = 0\n",
    "# print(\"\\n\\n\")\n",
    "# for cr_bin_key in tokens_CR_and_novelty_bins.keys():\n",
    "#     for novelty_bin_key in range(35, novelty_max_limit, 10):\n",
    "#         temp=len(tokens_CR_and_novelty_bins[cr_bin_key][novelty_bin_key])\n",
    "#         sc+=temp\n",
    "#         print(\"cr_bin_key = %d , novelty_bin_key = %d , bin_samples_count = %d , total_samples_counts = %d\"%(cr_bin_key,novelty_bin_key, temp, sc ) )\n",
    "      \n",
    "#     print(\"--------------------------------\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0\n",
      "1 0 0 0 1\n",
      "0 0 0 0 1\n",
      "1 0 0 0 2\n",
      "1 0 0 0 3\n",
      "1 0 0 0 4\n",
      "1 0 0 0 5\n",
      "9 0 0 7 14\n",
      "7 0 0 5 21\n",
      "11 1 1 8 30\n",
      "16 1 1 12 44\n",
      "36 3 3 28 74\n",
      "46 4 4 36 112\n",
      "34 3 3 27 140\n",
      "74 7 7 59 200\n",
      "83 8 8 66 267\n",
      "96 9 9 76 345\n",
      "134 13 13 107 453\n",
      "131 13 13 104 558\n",
      "152 15 15 121 680\n",
      "144 14 14 115 796\n",
      "322 32 32 257 1054\n",
      "349 34 34 279 1335\n",
      "415 41 41 332 1668\n",
      "437 43 43 349 2019\n",
      "459 45 45 367 2388\n",
      "388 38 38 310 2700\n",
      "320 32 32 256 2956\n",
      "634 63 63 507 3464\n",
      "810 81 81 648 4112\n",
      "777 77 77 621 4735\n",
      "809 80 80 647 5384\n",
      "699 69 69 559 5945\n",
      "544 54 54 435 6381\n",
      "390 39 39 312 6693\n",
      "840 84 84 672 7365\n",
      "947 94 94 757 8124\n",
      "979 97 97 783 8909\n",
      "860 86 86 688 9597\n",
      "710 71 71 568 10165\n",
      "492 49 49 393 10559\n",
      "295 29 29 236 10796\n",
      "13454 13454\n",
      "\n",
      "\n",
      "\n",
      "test_samples = 1329\n",
      "dev_samples = 1329\n",
      "train_samples = 10796\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "sc = 0 \n",
    "\n",
    "test_samples = []\n",
    "dev_samples = []\n",
    "train_samples = []\n",
    "\n",
    "\n",
    "for cr_bin_key in tokens_CR_and_novelty_bins.keys():\n",
    "    for novelty_bin_key in range(35, novelty_max_limit, 10):\n",
    "        temp = tokens_CR_and_novelty_bins[cr_bin_key][novelty_bin_key]\n",
    "        \n",
    "        data_split   = [10,10,80]# test, dev, train\n",
    "        test_count   = int((data_split[0]/100)*len(temp))\n",
    "        dev_count    = int((data_split[1]/100)*len(temp))\n",
    "        train_count  = int((data_split[2]/100)*len(temp))\n",
    "        \n",
    "        #print(len(temp),test_count, dev_count, train_count)\n",
    "        \n",
    "        test_samples.append(temp[:test_count])\n",
    "        dev_samples.append(temp[test_count: test_count+dev_count])\n",
    "        train_samples.append(temp[test_count+dev_count:])\n",
    "        sc+=len(temp)\n",
    "        print(len(temp),test_count, dev_count, train_count, len(list(itertools.chain(*train_samples))))\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "test_samples = list(itertools.chain(*test_samples))\n",
    "dev_samples = list(itertools.chain(*dev_samples))\n",
    "train_samples = list(itertools.chain(*train_samples))\n",
    "\n",
    "    \n",
    "print(sc ,sum([len(test_samples), len(dev_samples), len(train_samples)]))        \n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"test_samples = %d\"%len(test_samples))\n",
    "print(\"dev_samples = %d\"%len(dev_samples))\n",
    "print(\"train_samples = %d\"%len(train_samples))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_samples = 1329\n",
      "1329\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#destination = \"/home/priyanka/Desktop/Filtering/ex2_telugu_tokenizer/Equal_data_distribution/corpus/\"\n",
    "destination = \"/home/priyanka/Desktop/Filtering/ex2_telugu_tokenizer/Equal_data_distribution/backup/corpus/\"\n",
    "\n",
    "\n",
    "################################## Test ###################################\n",
    "te_a = open(destination+\"test.article.filter_.txt\",\"w\")\n",
    "te_s = open(destination+\"test.title.filter_.txt\",\"w\")\n",
    "\n",
    "print(\"test_samples = %d\"%len(test_samples))\n",
    "\n",
    "c = 0\n",
    "for i, sample in enumerate(test_samples):\n",
    "    article_content = \" \".join(dataset[sample]['article']['sentences_wise']).strip()\n",
    "    #article_content = article_content.lstrip()\n",
    "    #article_content = \" \".join(article_content)\n",
    "    \n",
    "    summary_content = \" \".join(dataset[sample]['summary']['sentences_wise']).strip()\n",
    "    #summary_content = summary_content.lstrip()\n",
    "    #summary_content = \" \".join(summary_content)\n",
    "    \n",
    "    article_content = article_content.replace(\"\\n\",\"\")\n",
    "    summary_content = summary_content.replace(\"\\n\",\"\")\n",
    "    \n",
    "    te_a.write(article_content)\n",
    "    te_a.write(\"\\n\")\n",
    "    \n",
    "    te_s.write(summary_content)\n",
    "    te_s.write(\"\\n\")\n",
    "    \n",
    "    if(len(article_content.split(\"\\n\"))>1 or len(summary_content.split(\"\\n\"))>1 ):\n",
    "        print(i, sample)\n",
    "        print(\"article_content: \",article_content.split(\"\\n\"),len(article_content.split(\"\\n\")))\n",
    "        print(\"\\n\")\n",
    "        print(\"summary_content: \",summary_content.split(\"\\n\"), len(summary_content.split(\"\\n\")))\n",
    "        print(\"\\n********************************************\\n\")\n",
    "    \n",
    "    c+=1\n",
    "    \n",
    "print(c)\n",
    "te_a.close()\n",
    "te_s.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_samples = 1329\n",
      "1329\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#destination = \"/home/priyanka/Desktop/Filtering/ex2_telugu_tokenizer/Equal_data_distribution/corpus/\"\n",
    "destination = \"/home/priyanka/Desktop/Filtering/ex2_telugu_tokenizer/Equal_data_distribution/backup/corpus/\"\n",
    "\n",
    "################################## Test ###################################\n",
    "te_a = open(destination+\"test.article.filter_m1.txt\",\"w\")\n",
    "te_s = open(destination+\"test.title.filter_m1.txt\",\"w\")\n",
    "\n",
    "print(\"test_samples = %d\"%len(test_samples))\n",
    "\n",
    "\n",
    "\n",
    "c = 0\n",
    "for i, sample in enumerate(test_samples):\n",
    "    article_content = \" \".join(dataset[sample]['article']['sentences_wise']).strip()\n",
    "    #article_content = article_content.lstrip()\n",
    "    #article_content = \" \".join(article_content)\n",
    "    \n",
    "    summary_content = \" \".join(dataset[sample]['summary']['sentences_wise']).strip()\n",
    "    #summary_content = summary_content.lstrip()\n",
    "    #summary_content = \" \".join(summary_content)\n",
    "    \n",
    "    article_content = article_content.replace(\"\\n\",\"\")\n",
    "    summary_content = summary_content.replace(\"\\n\",\"\")\n",
    "    \n",
    "    te_a.write(article_content)\n",
    "    te_a.write(\"\\n\")\n",
    "    \n",
    "    te_s.write(summary_content)\n",
    "    te_s.write(\"\\n\")\n",
    "    \n",
    "    sample_dir = \"m1/test/\"+sample ### Set wise creation:\n",
    "    if not os.path.exists(sample_dir): os.makedirs(sample_dir)\n",
    "    \n",
    "    article_name = \"article.\"+str(sample)+\".sent.txt\"#article.25276.sent.txt\n",
    "    summary_name = \"article.\"+str(sample)+\".summ.sent.txt\"#article.25276.summ.sent.txt\n",
    "\n",
    "    fa = open(sample_dir+\"/\"+article_name,\"w\")\n",
    "    fa.write(article_content)\n",
    "    fa.close()\n",
    "    \n",
    "    fs = open(sample_dir+\"/\"+summary_name,\"w\")\n",
    "    fs.write(summary_content)\n",
    "    fs.close()\n",
    "    \n",
    "    if(len(article_content.split(\"\\n\"))>1 or len(summary_content.split(\"\\n\"))>1 ):\n",
    "        print(i, sample)\n",
    "        print(\"article_content: \",article_content.split(\"\\n\"),len(article_content.split(\"\\n\")))\n",
    "        print(\"\\n\")\n",
    "        print(\"summary_content: \",summary_content.split(\"\\n\"), len(summary_content.split(\"\\n\")))\n",
    "        print(\"\\n********************************************\\n\")\n",
    "    \n",
    "    c+=1\n",
    "    \n",
    "print(c)\n",
    "te_a.close()\n",
    "te_s.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_samples = 1329\n",
      "1329\n",
      "train_samples = 10796\n",
      "10796\n",
      "dev_samples = 1329\n",
      "1329\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#destination = \"/home/priyanka/Desktop/Filtering/ex2_telugu_tokenizer/Equal_data_distribution/corpus/\"\n",
    "destination = \"/home/priyanka/Desktop/Filtering/ex2_telugu_tokenizer/Equal_data_distribution/backup/corpus/\"\n",
    "\n",
    "\n",
    "################################## Test ###################################\n",
    "te_a = open(destination+\"test.article.filter.txt\",\"w\")\n",
    "te_s = open(destination+\"test.title.filter.txt\",\"w\")\n",
    "\n",
    "print(\"test_samples = %d\"%len(test_samples))\n",
    "\n",
    "c = 0\n",
    "for sample in test_samples:\n",
    "    article_content = dataset[sample]['article']['content'].rstrip()\n",
    "    article_content = article_content.lstrip()\n",
    "    article_content = article_content.strip()\n",
    "    \n",
    "    summary_content = dataset[sample]['summary']['content'].rstrip()\n",
    "    summary_content = summary_content.lstrip()\n",
    "    summary_content = summary_content.strip()\n",
    "    \n",
    "    article_content = article_content.replace(\"\\n\",\"\")\n",
    "    summary_content = summary_content.replace(\"\\n\",\"\")\n",
    "\n",
    "    sample_dir = \"m1/test/\"+sample ### Set wise creation:\n",
    "    if not os.path.exists(sample_dir): os.makedirs(sample_dir)\n",
    "    article_name = \"article.\"+str(sample)+\".sent.txt\"#article.25276.sent.txt\n",
    "    summary_name = \"article.\"+str(sample)+\".summ.sent.txt\"#article.25276.summ.sent.txt\n",
    "\n",
    "    fa = open(sample_dir+\"/\"+article_name,\"w\")\n",
    "    fa.write(article_content)\n",
    "    fa.close()\n",
    "    \n",
    "    fs = open(sample_dir+\"/\"+summary_name,\"w\")\n",
    "    fs.write(summary_content)\n",
    "    fs.close()\n",
    "\n",
    "    \n",
    "    te_a.write(str(article_content))\n",
    "    te_a.write(\"\\n\")\n",
    "    \n",
    "    te_s.write(str(summary_content))\n",
    "    te_s.write(\"\\n\")\n",
    "    \n",
    "    c+=1\n",
    "    \n",
    "print(c)\n",
    "te_a.close()\n",
    "te_s.close()\n",
    "\n",
    "\n",
    "\n",
    "################################### Train ###################################\n",
    "print(\"train_samples = %d\"%len(train_samples))\n",
    "\n",
    "tr_a = open(destination+\"train.article.txt\",\"w\")\n",
    "tr_s = open(destination+\"train.title.txt\",\"w\")\n",
    "\n",
    "c = 0\n",
    "for sample in train_samples:\n",
    "    article_content = dataset[sample]['article']['content'].rstrip()\n",
    "    article_content = article_content.lstrip()\n",
    "    article_content = article_content.strip()\n",
    "    \n",
    "    summary_content = dataset[sample]['summary']['content'].rstrip()\n",
    "    summary_content = summary_content.lstrip()\n",
    "    summary_content = summary_content.strip()\n",
    "\n",
    "    \n",
    "    article_content = article_content.replace(\"\\n\",\"\")\n",
    "    summary_content = summary_content.replace(\"\\n\",\"\")\n",
    "\n",
    "    sample_dir = \"m1/train/\"+sample ### Set wise creation:\n",
    "    if not os.path.exists(sample_dir): os.makedirs(sample_dir)\n",
    "    article_name = \"article.\"+str(sample)+\".sent.txt\"#article.25276.sent.txt\n",
    "    summary_name = \"article.\"+str(sample)+\".summ.sent.txt\"#article.25276.summ.sent.txt\n",
    "\n",
    "    fa = open(sample_dir+\"/\"+article_name,\"w\")\n",
    "    fa.write(article_content)\n",
    "    fa.close()\n",
    "    \n",
    "    fs = open(sample_dir+\"/\"+summary_name,\"w\")\n",
    "    fs.write(summary_content)\n",
    "    fs.close()\n",
    "\n",
    "    \n",
    "    tr_a.write(str(article_content))\n",
    "    tr_a.write(\"\\n\")\n",
    "     \n",
    "    tr_s.write(str(summary_content))\n",
    "    tr_s.write(\"\\n\")\n",
    "    \n",
    "    c+=1\n",
    "    \n",
    "tr_a.close()\n",
    "tr_s.close()\n",
    "    \n",
    "print(c)\n",
    "\n",
    "################################### Dev ##################################\n",
    "print(\"dev_samples = %d\"%len(dev_samples))\n",
    "\n",
    "va_a = open(destination+\"valid.article.filter.txt\",\"w\")\n",
    "va_s = open(destination+\"valid.title.filter.txt\",\"w\")\n",
    "\n",
    "c = 0\n",
    "for sample in dev_samples:\n",
    "    article_content = dataset[sample]['article']['content'].rstrip()\n",
    "    article_content = article_content.lstrip()\n",
    "    article_content = article_content.strip()\n",
    "    \n",
    "    summary_content = dataset[sample]['summary']['content'].rstrip()\n",
    "    summary_content = summary_content.lstrip()\n",
    "    summary_content = summary_content.strip()\n",
    "    \n",
    "\n",
    "    article_content = article_content.replace(\"\\n\",\"\")\n",
    "    summary_content = summary_content.replace(\"\\n\",\"\")\n",
    "\n",
    "    sample_dir = \"m1/dev/\"+sample ### Set wise creation:\n",
    "    if not os.path.exists(sample_dir): os.makedirs(sample_dir)\n",
    "    article_name = \"article.\"+str(sample)+\".sent.txt\"#article.25276.sent.txt\n",
    "    summary_name = \"article.\"+str(sample)+\".summ.sent.txt\"#article.25276.summ.sent.txt\n",
    "\n",
    "    fa = open(sample_dir+\"/\"+article_name,\"w\")\n",
    "    fa.write(article_content)\n",
    "    fa.close()\n",
    "    \n",
    "    fs = open(sample_dir+\"/\"+summary_name,\"w\")\n",
    "    fs.write(summary_content)\n",
    "    fs.close()\n",
    "\n",
    "    \n",
    "    va_a.write(str(article_content))\n",
    "    va_a.write(\"\\n\")\n",
    "    \n",
    "    va_s.write(str(summary_content))\n",
    "    va_s.write(\"\\n\")\n",
    "    \n",
    "    c+=1\n",
    "    \n",
    "va_a.close()\n",
    "va_s.close()\n",
    "print(c)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### completed ######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
